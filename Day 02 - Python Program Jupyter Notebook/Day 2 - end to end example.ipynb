{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X</th>\n",
       "      <th>text</th>\n",
       "      <th>favorited</th>\n",
       "      <th>favoriteCount</th>\n",
       "      <th>replyToSN</th>\n",
       "      <th>created</th>\n",
       "      <th>truncated</th>\n",
       "      <th>replyToSID</th>\n",
       "      <th>id</th>\n",
       "      <th>replyToUID</th>\n",
       "      <th>statusSource</th>\n",
       "      <th>screenName</th>\n",
       "      <th>retweetCount</th>\n",
       "      <th>isRetweet</th>\n",
       "      <th>retweeted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>RT @rssurjewala: Critical question: Was PayTM ...</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11/23/16 18:40</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.014960e+17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/android\" ...</td>\n",
       "      <td>HASHTAGFARZIWAL</td>\n",
       "      <td>331</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>RT @Hemant_80: Did you vote on #Demonetization...</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11/23/16 18:40</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.014960e+17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/android\" ...</td>\n",
       "      <td>PRAMODKAUSHIK9</td>\n",
       "      <td>66</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>RT @roshankar: Former FinSec, RBI Dy Governor,...</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11/23/16 18:40</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.014960e+17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/android\" ...</td>\n",
       "      <td>rahulja13034944</td>\n",
       "      <td>12</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>RT @ANI_news: Gurugram (Haryana): Post office ...</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11/23/16 18:39</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.014960e+17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/android\" ...</td>\n",
       "      <td>deeptiyvd</td>\n",
       "      <td>338</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>RT @satishacharya: Reddy Wedding! @mail_today ...</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11/23/16 18:39</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.014950e+17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;a href=\"http://cpimharyana.com\" rel=\"nofollow...</td>\n",
       "      <td>CPIMBadli</td>\n",
       "      <td>120</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   X                                               text  favorited  \\\n",
       "0  1  RT @rssurjewala: Critical question: Was PayTM ...      False   \n",
       "1  2  RT @Hemant_80: Did you vote on #Demonetization...      False   \n",
       "2  3  RT @roshankar: Former FinSec, RBI Dy Governor,...      False   \n",
       "3  4  RT @ANI_news: Gurugram (Haryana): Post office ...      False   \n",
       "4  5  RT @satishacharya: Reddy Wedding! @mail_today ...      False   \n",
       "\n",
       "   favoriteCount replyToSN         created  truncated  replyToSID  \\\n",
       "0              0       NaN  11/23/16 18:40      False         NaN   \n",
       "1              0       NaN  11/23/16 18:40      False         NaN   \n",
       "2              0       NaN  11/23/16 18:40      False         NaN   \n",
       "3              0       NaN  11/23/16 18:39      False         NaN   \n",
       "4              0       NaN  11/23/16 18:39      False         NaN   \n",
       "\n",
       "             id  replyToUID  \\\n",
       "0  8.014960e+17         NaN   \n",
       "1  8.014960e+17         NaN   \n",
       "2  8.014960e+17         NaN   \n",
       "3  8.014960e+17         NaN   \n",
       "4  8.014950e+17         NaN   \n",
       "\n",
       "                                        statusSource       screenName  \\\n",
       "0  <a href=\"http://twitter.com/download/android\" ...  HASHTAGFARZIWAL   \n",
       "1  <a href=\"http://twitter.com/download/android\" ...   PRAMODKAUSHIK9   \n",
       "2  <a href=\"http://twitter.com/download/android\" ...  rahulja13034944   \n",
       "3  <a href=\"http://twitter.com/download/android\" ...        deeptiyvd   \n",
       "4  <a href=\"http://cpimharyana.com\" rel=\"nofollow...        CPIMBadli   \n",
       "\n",
       "   retweetCount  isRetweet  retweeted  \n",
       "0           331       True      False  \n",
       "1            66       True      False  \n",
       "2            12       True      False  \n",
       "3           338       True      False  \n",
       "4           120       True      False  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Importing necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import spacy\n",
    "import string\n",
    "# Reading the dataset\n",
    "df = pd.read_csv(\"demonetization-tweets.csv\", encoding= 'unicode_escape')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lower Casing\n",
    "It is the most common and simplest text preprocessing technique. Applicable to most text mining and NLP problems. The main goal is to convert the text into the lower casing so that ‘apple’, ‘Apple’ and ‘APPLE’ are treated the same way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    rt @rssurjewala: critical question: was paytm ...\n",
       "1    rt @hemant_80: did you vote on #demonetization...\n",
       "2    rt @roshankar: former finsec, rbi dy governor,...\n",
       "3    rt @ani_news: gurugram (haryana): post office ...\n",
       "4    rt @satishacharya: reddy wedding! @mail_today ...\n",
       "Name: text_lower, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lower Casing --> creating new column called text_lower\n",
    "df['text_lower']  = df['text'].str.lower()\n",
    "df['text_lower'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    rt rssurjewala critical question was paytm inf...\n",
       "1    rt hemant_80 did you vote on demonetization on...\n",
       "2    rt roshankar former finsec rbi dy governor cbd...\n",
       "3    rt ani_news gurugram haryana post office emplo...\n",
       "4    rt satishacharya reddy wedding mail_today cart...\n",
       "Name: text_punct, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Removal of Punctuations\n",
    "#removing punctuation, creating a new column called 'text_punct]'\n",
    "df['text_punct'] = df['text_lower'].str.replace('[^\\w\\s]','')\n",
    "df['text_punct'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stop-word removal\n",
    "Stop words are a set of commonly used words in a language. Examples of stop words in English are “a”, “we”, “the”, “is”, “are” and etc. The idea behind using stop words is that, by removing low information words from text, we can focus on the important words instead. We can either create a custom list of stopwords ourselves (based on use case) or we can use predefined libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    rt rssurjewala critical question paytm informe...\n",
       "1     rt hemant_80 vote demonetization modi survey app\n",
       "2    rt roshankar former finsec rbi dy governor cbd...\n",
       "3    rt ani_news gurugram haryana post office emplo...\n",
       "4    rt satishacharya reddy wedding mail_today cart...\n",
       "Name: text_stop, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Importing stopwords from nltk library\n",
    "from nltk.corpus import stopwords\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "# Function to remove the stopwords\n",
    "def stopwords(text):\n",
    "    return \" \".join([word for word in str(text).split() if word not in STOPWORDS])\n",
    "# Applying the stopwords to 'text_punct' and store into 'text_stop'\n",
    "df[\"text_stop\"] = df[\"text_punct\"].apply(stopwords)\n",
    "df[\"text_stop\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Common word removal\n",
    "We can also remove commonly occurring words from our text data First, let’s check the 10 most frequently occurring words in our text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('demonetization', 13956),\n",
       " ('rt', 11059),\n",
       " ('india', 2766),\n",
       " ('modi', 2759),\n",
       " ('pm', 2730),\n",
       " ('narendra', 1566),\n",
       " ('rich', 1509),\n",
       " ('find', 1422),\n",
       " ('dear', 1411),\n",
       " ('implement', 1399)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the first 10 most frequent words\n",
    "from collections import Counter\n",
    "cnt = Counter()\n",
    "for text in df[\"text_stop\"].values:\n",
    "    for word in text.split():\n",
    "        cnt[word] += 1\n",
    "        \n",
    "cnt.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can remove the frequent words in the given corpus. This can be taken care automatically if we use tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    rssurjewala critical question paytm informed e...\n",
       "1                            hemant_80 vote survey app\n",
       "2    roshankar former finsec rbi dy governor cbdt c...\n",
       "3    ani_news gurugram haryana post office employee...\n",
       "4    satishacharya reddy wedding mail_today cartoon...\n",
       "Name: text_common, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Removing the frequent words\n",
    "freq = set([w for (w, wc) in cnt.most_common(10)])\n",
    "# function to remove the frequent words\n",
    "def freqwords(text):\n",
    "    return \" \".join([word for word in str(text).split() if word not \n",
    "in freq])\n",
    "# Passing the function freqwords\n",
    "df[\"text_common\"] = df[\"text_stop\"].apply(freqwords)\n",
    "df[\"text_common\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OTHER OPTION - using NLTK TO GET FREQUECY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['In',\n",
       " 'Brazil',\n",
       " 'they',\n",
       " 'drive',\n",
       " 'on',\n",
       " 'the',\n",
       " 'right-hand',\n",
       " 'side',\n",
       " 'of',\n",
       " 'the',\n",
       " 'road',\n",
       " '.',\n",
       " 'Brazil',\n",
       " 'has',\n",
       " 'a',\n",
       " 'large',\n",
       " 'coastline',\n",
       " 'on',\n",
       " 'the',\n",
       " 'easternside',\n",
       " 'of',\n",
       " 'South',\n",
       " 'America']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CREATE TOKENS\n",
    "# Importing necessary library\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import os\n",
    "import nltk.corpus\n",
    "# sample text for performing tokenization\n",
    "text = \"In Brazil they drive on the right-hand side of the road. Brazil has a large coastline on the easternside of South America\"\n",
    "# importing word_tokenize from nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "# Passing the string text into word tokenize for breaking the sentences\n",
    "token = word_tokenize(text)\n",
    "token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'the': 3, 'Brazil': 2, 'on': 2, 'of': 2, 'In': 1, 'they': 1, 'drive': 1, 'right-hand': 1, 'side': 1, 'road': 1, ...})"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# finding the frequency distinct in the tokens\n",
    "# Importing FreqDist library from nltk and passing token into FreqDist\n",
    "from nltk.probability import FreqDist\n",
    "fdist = FreqDist(token)\n",
    "fdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 3),\n",
       " ('Brazil', 2),\n",
       " ('on', 2),\n",
       " ('of', 2),\n",
       " ('In', 1),\n",
       " ('they', 1),\n",
       " ('drive', 1),\n",
       " ('right-hand', 1),\n",
       " ('side', 1),\n",
       " ('road', 1)]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To find the frequency of top 10 words\n",
    "fdist1 = fdist.most_common(10)\n",
    "fdist1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rare word removal\n",
    "This is very intuitive, as some of the words that are very unique in nature like names, brands, product names, and some of the noise characters, such as html leftouts, also need to be removed for different NLP tasks. We also use a length of the words as a criteria for removing words with very a short length or a very long length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    rssurjewala critical question paytm informed e...\n",
       "1                            hemant_80 vote survey app\n",
       "2    roshankar former finsec rbi dy governor cbdt c...\n",
       "3    ani_news gurugram haryana post office employee...\n",
       "4    satishacharya reddy wedding mail_today cartoon...\n",
       "Name: text_rare, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Removal of 10 rare words and store into new column called 'text_rare'\n",
    "freq = pd.Series(' '.join(df['text_common']).split()).value_counts()[-10:] # 10 rare words\n",
    "freq = list(freq.index)\n",
    "df['text_rare'] = df['text_common'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq))\n",
    "df['text_rare'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spelling Correction\n",
    "Social media data always messy data and it has spelling mistakes. Hence, spelling correction is a useful pre-processing step because this will help us to avoid multiple words. Example, “text” and “txt” will be treated as different words even if they are used in the same sense. This can be done by textblob library\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    rssurjewala critical question part informed ed...\n",
       "1                             hemant_80 vote survey pp\n",
       "2    roshankar former finsen roi dy governor but ch...\n",
       "3    ani_news gurugram havana post office employees...\n",
       "4    satishacharya ready wedding mail_today cartoon...\n",
       "Name: text_rare, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Spell check using text blob for the first 5 records\n",
    "from textblob import TextBlob\n",
    "df['text_rare'][:5].apply(lambda x: str(TextBlob(x).correct()))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Emoji removal\n",
    "Emojis are part of our life. Social media text has a lot of emojis. We need to remove the same in our text analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hi, I am Emoji '"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function to remove emoji.\n",
    "def emoji(string):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', string)\n",
    "emoji(\"Hi, I am Emoji 😉\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#passing the emoji function to 'text_rare'\n",
    "df['text_rare'] = df['text_rare'].apply(emoji)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Emoticons removal\n",
    "In previous steps, we have removed emoji. Now, going to remove emoticons. What is the difference between emoji and emoticons? :-) is an emoticon and 😜 → emoji.\n",
    "Using emot library. Please refer more about emot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello '"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!pip install emot\n",
    "from emot.emo_unicode import UNICODE_EMO, EMOTICONS\n",
    "# Function for removing emoticons\n",
    "def remove_emoticons(text):\n",
    "    emoticon_pattern = re.compile(u'(' + u'|'.join(k for k in EMOTICONS) + u')')\n",
    "    return emoticon_pattern.sub(r'', text)\n",
    "remove_emoticons(\"Hello :-)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying remove_emoticons to 'text_rare'\n",
    "df['text_rare'] = df['text_rare'].apply(remove_emoticons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting Emoji and Emoticons to words\n",
    "In sentiment analysis, emojis and emoticons express an emotion. Hence, removing them might not be a good solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from emot.emo_unicode import UNICODE_EMO, EMOTICONS\n",
    "# Converting emojis to words\n",
    "\n",
    "def convert_emojis(text):\n",
    "    for emot in UNICODE_EMO:\n",
    "        text = text.replace(emot, \"_\".join(UNICODE_EMO[emot].replace(\",\",\"\").replace(\":\",\"\").split()))\n",
    "    return text\n",
    "# Function for converting emoticons into word\n",
    "def convert_emoticons(text):\n",
    "    for emot in EMOTICONS:\n",
    "        text = re.sub(u'('+emot+')', \"_\".join(EMOTICONS[emot].replace(\",\",\"\").split()), text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello Happy_face_smiley Happy_face_smiley'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example\n",
    "text = \"Hello :-) :-)\"\n",
    "convert_emoticons(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hilarious face_with_tears_of_joy'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1 = \"Hilarious 😂\"\n",
    "convert_emojis(text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Passing both functions to 'text_rare'\n",
    "df['text_rare'] = df['text_rare'].apply(convert_emoticons)\n",
    "df['text_rare'] = df['text_rare'].apply(convert_emojis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removal of URL’s\n",
    "Removing URLs in the text. We can use Beautiful soup library\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for url's\n",
    "def remove_urls(text):\n",
    "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return url_pattern.sub(r'', text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is my website, '"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Examples\n",
    "text = \"This is my website, https://www.abc.com\"\n",
    "remove_urls(text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Passing the function to 'text_rare'\n",
    "df['text_rare'] = df['text_rare'].apply(remove_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removal of HTML tags\n",
    "Another common preprocessing technique is removing HTML tags. HTML tags usually presented in scraping data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " This\n",
      " is\n",
      " ABCD\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "#Function for removing html\n",
    "def html(text):\n",
    "    return BeautifulSoup(text, \"lxml\").text\n",
    "# Examples\n",
    "text = \"\"\"<div>\n",
    "<h1> This</h1>\n",
    "<p> is</p>\n",
    "<a href=\"https://www.abc.com/\"> ABCD</a>\n",
    "</div>\n",
    "\"\"\"\n",
    "print(html(text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Passing the function to 'text_rare'\n",
    "df['text_rare'] = df['text_rare'].apply(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming and Lemmatization\n",
    "Lemmatization is the process of converting a word to its base form. The difference between stemming and lemmatization is, lemmatization considers the context and converts the word to its meaningful base form, whereas stemming just removes the last few characters, often leading to incorrect meanings and spelling errors. Here, lemmatization only performed. We need to provide the POS tag of the word along with the word for lemmatizer in NLTK. Depending on the POS, the lemmatizer may return different results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "wordnet_map = {\"N\":wordnet.NOUN, \"V\":wordnet.VERB, \"J\":wordnet.ADJ, \"R\":wordnet.ADV} # Pos tag, used Noun, Verb, Adjective and Adverb\n",
    "# Function for lemmatization using POS tag\n",
    "def lemmatize_words(text):\n",
    "    pos_tagged_text = nltk.pos_tag(text.split())\n",
    "    return \" \".join([lemmatizer.lemmatize(word, wordnet_map.get(pos[0], wordnet.NOUN)) for word, pos in pos_tagged_text])\n",
    "# Passing the function to 'text_rare' and store in 'text_lemma'\n",
    "df[\"text_lemma\"] = df[\"text_rare\"].apply(lemmatize_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
