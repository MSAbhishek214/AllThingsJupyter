{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a class to encapsulate all the cleaning functions\n",
    "\n",
    "class Cleaner:\n",
    "    '''\n",
    "    This class contains functions to clean the input data provided (ideally a text file)\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    def __init__(self, text_file):\n",
    "        '''\n",
    "        This is the init constructor function that will initialize the input text data to that particular\n",
    "        instance of the class.\n",
    "        \n",
    "        args:\n",
    "        \n",
    "        text_file = the input string data.\n",
    "        '''\n",
    "        \n",
    "        self.text_file = text_file\n",
    "        \n",
    "        \n",
    "    def punctuation_removal(self, sentences):\n",
    "        '''\n",
    "        Function to remove punctuation and special characters from the raw data.\n",
    "        '''\n",
    "        \n",
    "        import pandas as pd\n",
    "        clean_sentences = pd.Series(sentences).str.replace(\"[^a-zA-Z]\", \" \")\n",
    "        return clean_sentences\n",
    "        \n",
    "        \n",
    "        \n",
    "#     def stopword_removal(self, text_file):\n",
    "#         '''\n",
    "#         Function to remove stopwords from the input raw data.\n",
    "#         '''\n",
    "        \n",
    "#         print('able to access stopword_removal function and run it')\n",
    "        \n",
    "        \n",
    "    def tokenize_sentences(self, text_file):\n",
    "        '''\n",
    "        Function to tokenize the given paragraph to sentences.\n",
    "        '''\n",
    "        \n",
    "        from nltk.tokenize import sent_tokenize\n",
    "        sentences = []\n",
    "        for s in text_file:\n",
    "            sentences.append(sent_tokenize(s))\n",
    "\n",
    "        sentences = [y for x in sentences for y in x]\n",
    "        return sentences\n",
    "        \n",
    "        \n",
    "    def extract_feature(self, clean_sentences):\n",
    "        from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "        tf_idf_vec = TfidfVectorizer(use_idf=True, \n",
    "                        smooth_idf=False,  \n",
    "                        ngram_range=(1,1),stop_words='english')\n",
    "        tf_idf_matrix = tf_idf_vec.fit_transform(clean_sentences)\n",
    "        return tf_idf_matrix\n",
    "\n",
    "    \n",
    "    def cosine_similarity(self, tf_idf_matrix):\n",
    "        from sklearn.metrics.pairwise import cosine_similarity\n",
    "        import numpy as np\n",
    "        sim_mat = np.zeros([len(clean_sentences), len(clean_sentences)])\n",
    "        cosine_sim = cosine_similarity(tf_idf_matrix, tf_idf_matrix)\n",
    "        for i in range(len(sentences)):\n",
    "            for j in range(len(sentences)):\n",
    "                if i != j:\n",
    "                    sim_mat[i][j] = cosine_sim[i][j]\n",
    "        return sim_mat\n",
    "    \n",
    "    \n",
    "    def page_ranking(self, sim_mat):\n",
    "        import networkx as nx\n",
    "\n",
    "        nx_graph = nx.from_numpy_array(sim_mat)\n",
    "        scores = nx.pagerank(nx_graph)\n",
    "        return scores\n",
    "        \n",
    "        \n",
    "    def show_summary(self, scores):\n",
    "        ranked_sentences = sorted(((scores[i],s) for i,s in enumerate(clean_sentences)), reverse=True)\n",
    "        for i in range(2):\n",
    "            print(ranked_sentences[i][1])\n",
    "  \n",
    "        \n",
    "    def run_summarizer(self, text_file):\n",
    "        self.tokenize_sentences(text_file)\n",
    "        self.punctuation_removal(text_file)\n",
    "        self.extract_feature(text_file)\n",
    "        self.cosine_similarity(text_file)\n",
    "        #self.page_ranking(similarity_matrix)\n",
    "        #self.show_summary(ranked_sentences)\n",
    "        print('All functions run finished!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ['\\n\\nthe fresh initiative by the union telecommunications ministry to tackle pesky calls is welcome. cracking down on such unwelcome calls, which are a source of great irritation to telecom subscribers, comes after earlier attempts like setting up the do not disturb registry have been bypassed by telemarketers. with a large army of educated, unemployed youth cheaply feeding into the staffing needs of telemarketers, the number of pesky calls have multiplied manifold. in the absence of monetary penalties and other regulatory deterrence, there was no incentive on the part of telemarketers to change their ways.\\n\\nthe digital intelligence unit to investigate mobile frauds will also have its hands full. for one, the linkages between telemarketers and those fraudulently peddling financial products has to be more thoroughly investigated. the recent crackdown on mobile lending apps revealed that these used fully equipped call centres to act as collection agents to force debtors to repay.\\nread also: govt makes fresh bid to rein in pesky callers, phone frauds\\non top of this there is today easy access to personal data, which makes it simple for telemarketers to commit financial fraud on gullible consumers, especially the elderly. a data protection law to prevent the leakage of consumer telecom and financial information in the possession of telecom and banking companies is also necessary.the new digital intelligence unit must effectively invoke penalties and choke telecom resources to those abusing teleconnectivity. otherwise, there will be no end to this menace.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = ' '.join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = Cleaner(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      the fresh initiative by the union telecommun...\n",
       "1    cracking down on such unwelcome calls  which a...\n",
       "2    with a large army of educated  unemployed yout...\n",
       "3    in the absence of monetary penalties and other...\n",
       "4    the digital intelligence unit to investigate m...\n",
       "5    for one  the linkages between telemarketers an...\n",
       "6    the recent crackdown on mobile lending apps re...\n",
       "7    read also  govt makes fresh bid to rein in pes...\n",
       "8    a data protection law to prevent the leakage o...\n",
       "9      otherwise  there will be no end to this menace \n",
       "dtype: object"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = summary.tokenize_sentences(text)\n",
    "summary.punctuation_removal(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_summarizer():\n",
    "        summary.tokenize_sentences(text_file)\n",
    "        summary.punctuation_removal(text_file)\n",
    "        summary.extract_feature(text_file)\n",
    "        summary.cosine_similarity(text_file)\n",
    "        summary.page_ranking(similarity_matrix)\n",
    "        print('All functions run finished!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
